import json
from typing import Dict, Any, Optional
import google.generativeai as genai

class FeedbackAndReflectionAgent:
    """
    An agent that reflects on a generated answer to provide critique and suggest improvements.
    It helps in self-correction by evaluating the answer's quality, faithfulness to the context,
    and relevance to the query.
    """

    def __init__(self, gemini_api_key: str, model_name: str = "gemini-1.5-flash"):
        """
        Initializes the Feedback and Reflection Agent.

        Args:
            gemini_api_key: The API key for the Gemini model.
            model_name: The specific Gemini model to use.
        """
        self.gemini_api_key = gemini_api_key
        self.model_name = model_name
        self._model = None
        self._initialize_llm()

    def _initialize_llm(self):
        """Initialize the Gemini model with the provided API key."""
        genai.configure(api_key=self.gemini_api_key)
        self._model = genai.GenerativeModel(self.model_name)

    def reflect_on_answer(self, query: str, context: Dict, initial_answer: str) -> Dict[str, Any]:
        """
        Reflects on a given answer and provides a critique and potential improvements.

        Args:
            query: The original user query.
            context: The context (including retrieved chunks) used to generate the answer.
            initial_answer: The answer generated by a previous agent.

        Returns:
            A dictionary containing the critique, a quality score, and a suggested improved answer.
        """
        prompt = f"""
        You are a meticulous Reflection and Quality Assurance agent. Your task is to critique a generated answer based on the user's query and the provided context.

        **User Query:**
        {query}

        **Provided Context:**
        {json.dumps(context, indent=2)}

        **Initial Answer to Evaluate:**
        {initial_answer}

        **Your Task:**
        1.  **Critique:** Assess the answer's quality. Is it accurate, faithful to the context, comprehensive, and directly responsive to the query? Note any hallucinations or missing information.
        2.  **Score:** Rate the answer on a scale of 1 to 5, where 5 is best.
        3.  **Suggest Improvement:** Based on your critique, provide a revised and improved answer. If the initial answer is already perfect, you can state that.

        Respond with a JSON object containing:
        - "critique": (string) Your detailed critique.
        - "score": (integer) Your quality score from 1 to 5.
        - "improved_answer": (string) The revised answer.
        """
        try:
            response = self._model.generate_content(prompt)
            text = response.text.strip().lstrip("```json\n").rstrip("```")
            reflection = json.loads(text)
            return reflection
        except Exception as e:
            print(f"Error during reflection: {e}")
            return {"critique": "Failed to generate reflection.", "score": 0, "improved_answer": initial_answer}